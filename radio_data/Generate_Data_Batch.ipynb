{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Datasets for Training and Testing on a single machine\n",
    "\n",
    "## BATCH VERSION\n",
    "\n",
    "In this notebook, we report the code to generate datasets for training and testing the neural network models.\n",
    "The datasets contain channel realizations of a realistic LTE link operating over an industry-standard radio channel model.\n",
    "\n",
    "**Note**: Running this code on a sigle machine might be computationally heavy, therefore we suggest the reader to use a cluster of machine if possible. Please refer to \"radio_data/Generate_Data_Distributed.ipynb\", in which the same code as in this notebook is structured to be run on a cluster of machines using the package `ray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itpp\n",
    "import ray\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src import TDL_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _run_link_simulation(block_size, \n",
    "                         modorder,\n",
    "                         nrof_subcarriers,\n",
    "                         snr, \n",
    "                         channel_coeff):\n",
    "    \n",
    "    from src import single_link_bicm_ofdm\n",
    "\n",
    "    channel_block_fading = np.tile(np.transpose(channel_coeff), (12, 1))\n",
    "    \n",
    "    return single_link_bicm_ofdm.simulate(block_size, \n",
    "                                          modorder,\n",
    "                                          nrof_subcarriers,\n",
    "                                          snr, \n",
    "                                          channel_block_fading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Channel Generation\n",
    "fft_size       = 128\n",
    "channel_model  = 'ITU_VEHICULAR_B'\n",
    "relative_speed = 33.33 # m/s\n",
    "\n",
    "nrof_subcarriers = 72\n",
    "snrs_db        = [5]\n",
    "\n",
    "TRANSPORT_BLOCK_SIZES = [152, 200, 248, 320, 408, 504, 600, 712, 808, 936, \n",
    "                         936, 1032, 1192, 1352, 1544, 1736, 1800, \n",
    "                         1800, 1928, 2152, 2344, 2600, 2792, 2984, 3240, 3496, 3624, 3752, 4008]\n",
    "MODULATION_ORDERS     = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, \n",
    "                         4, 4, 4, 4, 4, 4, 4, \n",
    "                         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nrof_snrs = len(snrs_db)\n",
    "nrof_samples = 1000\n",
    "nrof_batches = 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Generate the channel realizations'''\n",
    "\n",
    "channel_coeff = np.ndarray((nrof_samples, nrof_subcarriers, nrof_batches), dtype=np.complex128)\n",
    "\n",
    "if channel_model != 'AWGN':\n",
    "    for batch_index in range(nrof_batches):\n",
    "        channel_response = TDL_channel.channel_frequency_response( fft_size,\n",
    "                                                                   relative_speed,\n",
    "                                                                   channel_model,\n",
    "                                                                   nrof_samples )\n",
    "\n",
    "        channel_coeff[:,:,batch_index] = channel_response.T().to_numpy_ndarray()[:, :nrof_subcarriers]\n",
    "else:\n",
    "    channel_coeff[:, :, :] = 1\n",
    "\n",
    "legend_strings = []\n",
    "\n",
    "block_success_dataset = np.ndarray((nrof_samples, len( TRANSPORT_BLOCK_SIZES ), nrof_batches))\n",
    "channel_to_noise_ratio_dataset = np.ndarray((nrof_subcarriers, nrof_samples, nrof_batches))\n",
    "\n",
    "start = time.time()\n",
    "for block_size_index in range( len( TRANSPORT_BLOCK_SIZES ) ):\n",
    "\n",
    "    block_size = TRANSPORT_BLOCK_SIZES[ block_size_index]  + 24 # 24 bit CRC\n",
    "    modorder = MODULATION_ORDERS[ block_size_index ]\n",
    "\n",
    "    outcome = [_run_link_simulation(block_size, \n",
    "                                    modorder,\n",
    "                                    nrof_subcarriers,\n",
    "                                    snrs_db[0], \n",
    "                                    channel_coeff[:,:,i]) for i in range(nrof_batches)]\n",
    "\n",
    "    for batch_index in range(nrof_batches):\n",
    "        block_success_dataset[:, block_size_index, batch_index] = outcome[batch_index][1]\n",
    "\n",
    "    print('Block size index %d, TBS %d, Elapsed: %0.2fs' %(block_size_index, \n",
    "                                                           block_size, \n",
    "                                                           time.time() - start))\n",
    "\n",
    "FADING_CHANNEL_DATASET = {'channel': channel_coeff, \n",
    "                          'block_success': block_success_dataset,\n",
    "                          'block_sizes': TRANSPORT_BLOCK_SIZES,\n",
    "                          'snrs_db': snrs_db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FADING_CHANNEL_DATASET = {'channel': channel_coeff, \n",
    "                          'block_success': block_success_dataset,\n",
    "                          'block_sizes': TRANSPORT_BLOCK_SIZES,\n",
    "                          'snrs_db': snrs_db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_filepath = 'sim_data/sim_0001/'\n",
    "data_filename = '%s_%d_%d_%0.2f_%d_%ddB.npy'%(channel_model, nrof_samples, nrof_batches, relative_speed, nrof_subcarriers, snrs_db[0])\n",
    "#data_filename = 'dataset.npy'  \n",
    "\n",
    "if not os.path.exists(data_filepath):\n",
    "    os.makedirs(data_filepath)  \n",
    "\n",
    "np.save(data_filepath + data_filename, FADING_CHANNEL_DATASET)\n",
    "\n",
    "print('Saved generated dataset to %s%s'%(data_filepath, data_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
